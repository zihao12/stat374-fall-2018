---
title: "hw1"
author: "Zihao_Wang"
date: "10/7/2018"
output: pdf_document
---

```{r}
rm(list=ls())
set.seed(12345)
options(warn = -1)
knitr::opts_knit$set(root.dir = '~/Desktop/stat374-fall-2018/analysis/')
library(kedd)
library(locfit)
library(gridExtra)
library(reshape)
library(gam)
library(tidyverse)
```


# 1. Computing and plotting with R

## (a)
```{r}
## function to compute empirical mean square error 
mse <- function(n,sigma){
  mysample = rnorm(n = n, sd = sigma, mean = 1)
  return((mean(mysample)-1)^2)
}

emse <- function(n,sigma,B){
  return(mean(replicate(B,mse(n,sigma))))
}

## simulate and plot
sigma = 1
B = 100

ns = seq(1,1000,5)
results = replicate(length(ns),0)
theory = replicate(length(ns),0)

for(i in 1:length(ns)){
  results[i] = emse(ns[i],sigma,B)
  theory[i] = sigma^2/ns[i]
}

plot(log(ns), log(results), xlab = "n", ylab = "mse")
lines(log(ns),log(theory), col = "red")
```

### Comment:
From the simulation experiments, we can see the results align with theoretical results

## (b)

```{r}
n = 1000
sigma = 1
B = 10000

## simulation
Z = replicate(B,sqrt(n)*(mean(rnorm(n,1,sigma)) - 1))

## theoretical standard normal
ns = seq(-10,10,0.01)
theory = replicate(length(ns),0)
for(i in 1:length(ns)){
  theory[i] = 1/sqrt(2*pi) * exp(-ns[i]^2 *0.5)
}

plot(density(Z), col = "blue")
lines(ns,theory,col = "red")
```
### Comment:
When B is too small (say, only 100), **density(B)** does not give very good estimate. When B is big enough, density estimation is quite good.



# 2 Leave-oue-out cross-validation

## (a)
By definition, $\hat{R}(h) = \frac{1}{n} * \sum_{i = 1}^{n} (r(x_{i})-\hat{r}_{-i}(x_{i}))^2$. 
Then we have $r(x_{i}) - \hat{r}_{-i}(x_{i}) = Y_{i} - \frac{\sum_{k \neq i} L_{i,k} * Y_{k}}{1-L_{ii}} = \frac{Y_{i} - \hat{r}_{n}(x_{i})} {1-L  _{ii}}$. Then our desired equation follows.

## (b)
Note: I select bandwidth in two ways: coding the loocv(bandwidth) function myself, and using the gcv function in the package. The two results are quite different, so I show both of them.

### See what the data looks like
```{r}
Doppler <- function(x){
  y = sqrt(x*(1-x)) * sin(2.1*pi/(x+0.05)) 
  return(y)
}

N = 1000
sigma = 0.1
Xs = seq(0,1, length = N)
ys_true = Doppler(Xs)
ys = ys_true + rnorm(N,sd = sigma)
plot(Xs,ys)
lines(Xs,ys_true, col = "red")
```

### Plot Cross-validation-score vs bandwidth
```{r}
K <- function(x){
  return(1/sqrt(2*pi) * exp(-x^2/2))
}

Rhat <- function(h,Xs){
  ### compute L (L_ij = l_j(x_i), rowSums(L) = 1,1...)
  n = length(Xs)
  X_matrix = replicate(n,Xs)
  X_difference_scaled = (X_matrix - t(X_matrix))/h 
  X_difference_scaled_kernel = K(X_difference_scaled) ## X_ij = K((Xi-Xj)/h)
  L = diag(1/rowSums(X_difference_scaled_kernel)) %*% X_difference_scaled_kernel 
  
  ### Compute Lii
  L_diag = diag(L)
  
  ### Compute ys_hat
  ys_hat = L %*% ys
  
  ### Conmpute R_hat
  R_hat = mean(((ys-ys_hat)/(1-L_diag))^2)
  
  return(R_hat)
}
```


```{r}
hs = seq(0,0.05,0.001)
myloocvs <- readRDS("../data/loocvs_hw1_p2") ## I save the computed results as it will take some time
#myloocvs <- sapply(hs, function(h) Rhat(h,Xs)) 
plot(myloocvs)
#saveRDS(myloocvs,"../data/loocvs_hw1_p2.rds")
## We can also find the best bandwidth using gcv function, as an approximate (however, it gives me quite different results)
# gcvs = gcvplot(ys~Xs, alpha = seq(0,0.5,0.01))
# plot(gcvs$alpha, gcvs$values)
```

### locfit using the optimal bandwidth
```{r}
h_star = hs[which.min(myloocvs)]
#h_star = 0.02
locfitopt = locfit(ys~Xs,alpha = h_star) ## if the first the element is 0, it reports error! How does this affect estimation??
plot(Xs, ys)
lines(Xs,predict(locfitopt, newdata = Xs), type = "l",col = "red")
#lines(xg, Doppler(xg), col = "blue")
```
### Comment:
This should be the optimal, but from the plot it seems to be undersmoothing a lot (especially compared with the results using gcv, which will be shown later)

## compute and plot the confidence interval for r(x)
```{r}
## Formula: rhat(x) +- Z_a/2 * sigmahat(x) * |l(x)|
## Assumption: homoscedasticity of variance
## Formula for sigamhat(x): sigmahat(x) = sum(residue^2) / (n - 2*nu - 2*nutilde), where nu = tr(L), nutilde = tr(L^t*T)
## they can be retrived in dp1, dp2 respectively

nu = locfitopt$dp[["df1"]]
nutilde = locfitopt$dp[["df2"]]
sigmahat_sqrt = sum(residuals(locfitopt)^2)/(N - 2*nu + nutilde)

diaghat = predict(locfitopt,where="data",what="infl") ## L_ii
normell = predict(locfitopt,where="data",what="vari") ## |l_i(x)|

critval = 1.96
xg = Xs
pred = predict(locfitopt, newdata = xg)

width = critval * sqrt(sigmahat_sqrt*normell)
upper = pred + width
lower = pred - width


## plot confidence band
plot(xg, pred,lwd=3, xlab="l",ylab="Cl",cex=3,cex.axis=1.3, cex.lab=1.3,type="l")
lines(xg, upper,col = 2, lwd = 3)
lines(xg, lower,col = 3, lwd = 2)



```

## Select bandwidth using gcv function & Fit with optimal h
```{r}
## use GCV to choose bandwidth
alphamat = matrix(0,ncol = 2, nrow = 100)
alphamat[,2] = seq(0,1, length = 100)

gcvs = gcvplot(ys~Xs,alpha = alphamat)
plot(gcvs$alpha[,2], gcvs$values)

optband2 = gcvs$alpha[which.min(gcvs$values),2]
locfitopt2 = locfit(ys~Xs,alpha= c(0,optband2))

plot(Xs, ys)
lines(Xs,predict(locfitopt2, newdata = Xs), type = "l",col = "red")

nu = locfitopt2$dp[["df1"]]
nutilde = locfitopt2$dp[["df2"]]
sigmahat_sqrt = sum(residuals(locfitopt2)^2)/(N - 2*nu + nutilde)

diaghat = predict(locfitopt2,where="data",what="infl") ## L_ii
normell = predict(locfitopt2,where="data",what="vari") ## |l_i(x)|

critval = 1.96
xg = Xs
pred = predict(locfitopt2, newdata = xg)

width = critval * sqrt(sigmahat_sqrt*normell)
upper = pred + width
lower = pred - width


## plot confidence band
plot(xg, pred,lwd=3, xlab="l",ylab="Cl",cex=3,cex.axis=1.3, cex.lab=1.3,type="l")
lines(xg, upper,col = 2, lwd = 3)
lines(xg, lower,col = 3, lwd = 2)

```

## Is In(x) the 95% CI for $r(x)$
No. In(x) is centered around $E(\hat{r}(x))$, which is not $r(x)$. This CI is computed by the fact that for a fixed x, $\hat{r}(x)$ asympototically follows $N(E(\hat{r}(x)), \hat{\sigma}(x))$. That correctness of the claim requires $E(\hat{r}(x)) = r(x)$, which is apparently violated.  



# 3. Kernel density estimate for Old Faithful Geyser
```{r}
data("faithful")
attach(faithful)

library(kedd)
## Broad search
ucv_eruptions = h.ucv(eruptions)
plot(ucv_eruptions)
## fine search (locate a neighborhood of the best point from broad search)
ucv_eruptions = h.ucv(eruptions, lower = 0.7*ucv_eruptions$h, higher = 1.3*ucv_eruptions$h, tol = 0.0001)

## Broad search
ucv_waiting = h.ucv(waiting)
plot(ucv_waiting)
## fine search (locate a neighborhood of the best point from broad search)
ucv_waiting = h.ucv(waiting, lower = 0.8*ucv_waiting$h, higher = 1.2*ucv_waiting$h,tol = 0.0001)

```

##  plot estimated density with optimum h
```{r}
plot(density(eruptions, bw = ucv_eruptions$h))
plot(density(waiting, bw = ucv_waiting$h))

```

# 4 Risk of a two-dimensional Kernel density estimate

Let $K_h(x,y,X,Y) := \frac{1}{h^2} E(K(\frac{X-x}{h})K(\frac{Y-y}{h}))$

By $|\Pr(x,y) - p(x-uh,y-vh)  < L(|uh|^\beta + |vh|^\beta|)$, we have $\Pr(x-uh,y-uh) < \Pr(x,y) + Lh^\beta(|u|^\beta+|v|^\beta)$

Thus:
$$
E(\hat{\Pr(x,y)}) = \frac{1}{h^2} E(K_h(x,y,X,Y)) \\
= \frac{1}{h^2} \int K(\frac{X-x}{h}) K(\frac{Y-y}{h}) \Pr(X,Y)dXdY \\
= \int K(u) K(v) P(x-Uh, y-Vh)dUdV \\
(U := \frac{x-X}{h})\\
\le \int K(u) K(v) (P(x,y) + Lh^\beta(|u|^\beta+|v|^\beta))dUdV \\
= \Pr(x,y) + Lh^\beta \int K(u)K(v)(|u|^\beta+|v|^\beta)dUdV\\
= \Pr(x,y) + Lh^\beta M \\
(M := \int K(u)K(v)(|u|^\beta+|v|^\beta)dUdV)
$$
Thus we have $bias(x,y) = E(\hat{P_n}(x,y)) - \Pr(x,y) <= MLh^\beta$

Similarly, for variance,
$$
Var(\hat{P_n}(x,y)) = \frac{1}{n} Var(K_h(x,y,X,Y))\\
\leq \frac{1}{n} E(K_h(x,y,X,Y)^2)\\
= \frac{1}{nh^4} \int K^2(\frac{X-x}{h}) K^2(\frac{Y-y}{h}) \Pr(X,Y)dXdY \\
= \frac{1}{nh^2} \int K^2(U) K^2(V) \Pr(x-Uh,y-Vh)dUdV \\
\leq \frac{1}{nh^2} (\int K^2(U) K^2(V)dUdV + Lh^\beta \int K^2(U)K^2(V) (|U|^\beta + |V|^\beta))dUdV)\\
\leq \frac{M_2}{nh^2} \Pr(x,y)\\
(M_2 := \frac{1}{nh^2} (\int K^2(U) K^2(V)dUdV)
$$

Thus we have $V(x,y) \leq \frac{M_2\Pr(x,y)}{nh^2}$\

It is also easy to check both $M$ and $M_2$ are not inifinity.\

$$
Risk(x,y) = bias(x,y)^2 + V(x,y) \leq (MLh^\beta)^2 + \frac{M_2\Pr(x,y)}{nh^2} := H(h) \\
h^* = (\frac{M_2\Pr(x,y)}{ML^2}) = O((\frac{1}{n})^\frac{1}{2\beta+2})\\
H(h^*) = O(\frac{1}{n})^\frac{\beta}{\beta+1}
$$
Thus set $H(h^*) < \epsilon$, we have the rate of convergence as $O(\epsilon ^ -\frac{\beta+1}{\beta})$



# 5  Capital Bike Sharing
```{r}
data = read.csv("../data/hw1/train.csv")
test = read.csv("../data/hw1/test.csv")

#fac_names = c("holiday","workingday","weather")
# fac_names = c("holiday","workingday","weather","season","year","hour")
fac_names = c("holiday","workingday","weather","season","year","hour")

for(i in 1:length(fac_names)){
  name = fac_names[i]
  data[[name]] = factor(data[[name]])
  test[[name]] = factor(test[[name]])
}
data[["transformed_count"]] = log(data$count + 1)

## split train into train and validation
train = data[data$day < 16,]
val = data[data$day > 15,]

## define loss
RMSLE_log <- function(count_log,count_hat_log){
  return(sqrt(mean((count_log-count_hat_log)^2)))
}

```

```{r}
#plot(density(train$count))
hist(train$count)
```

```{r}
# p1 = ggplot(train,aes(atemp, count, color = weather)) + 
#   geom_point()
# 
# p2 = ggplot(train,aes(humidity,count,  color = weather)) + 
#   geom_point()
# 
# p3 = ggplot(train,aes(windspeed,count,  color = weather)) + 
#   geom_point()

p1 <- ggplot(subset(train,workingday==1),aes(hour,count)) + 
  geom_point()

p2 <- ggplot(subset(train,workingday==0),aes(hour,count)) + 
  geom_point()

grid.arrange(p1,p2,nrow = 2)

```
### Comment:
The counts distribution with hours are different for workingday and nonworkingday. So there is a very strong connection between the interaction between hour and workingday! Other factors do not bring about too much difference

## (a) linear model on count
```{r}
linearMD <-lm(transformed_count~daylabel+workingday*hour + season +atemp+humidity+windspeed,data=train)
summary(linearMD)
#plot(linearMD)
```

```{r}
# linearPredict = predict(linearMD, subset(val, colnames = c("atemp","humidity","windspeed","weather")))
linearPredict_train = predict(linearMD, train)
linearPredict_val = predict(linearMD, val)



print(paste0("training loss: ", RMSLE_log(linearPredict_train,train$transformed_count)))
print(paste0("validation loss: ", RMSLE_log(linearPredict_val,val$transformed_count)))

plot(val$transformed_count[1:100])
lines(linearPredict_val,col = "blue")

```

### Comment:
Our model for linear regression is: $Y = X*\beta + \epsilon$.
The normality assumption holds, but the residue seems not to be independent of X. Also, the R-squared is only around 25%, meaning our model does not account for much variance in data. p-value suggests that we should reject the null hypothesis that the selected variables are not correlated with counts.


## (b)
First, summarize the data by mean hourly counts
```{r warning=FALSE}
varnames = dimnames(train)[[2]]
ids = varnames[varnames != "transformed_count"]

## melt data
val_mlt = melt(val,id = ids)
train_mlt = melt(train,id = ids)

train_mlt$value = as.numeric(train_mlt$value)
train_hourmean = cast(train_mlt,daylabel~variable,mean)

attach(train_hourmean)
plot(daylabel,transformed_count)
```


```{r warning=FALSE}
attach(train_hourmean)

locfitmodel_hourmean_train = locfit(transformed_count~daylabel)
predict_hourmean_train = predict(locfitmodel_hourmean_train,daylabel)
plot(daylabel,predict_hourmean_train)

train_hourmean_trend = train_hourmean
train_hourmean_trend$transformed_count = predict_hourmean_train

train_hourmean_residue = train_hourmean
train_hourmean_residue$transformed_count = residuals(locfitmodel_hourmean_train)
detach(train_hourmean)
```

```{r warning=FALSE}
u_daylabels = unique(train_mlt$daylabel)

## get residue as new response
train_residue = train

for(i in 1:length(u_daylabels)){
  train_residue[train_residue$daylabel == u_daylabels[i],"transformed_count"] =  train_residue[train_residue$daylabel == u_daylabels[i],"transformed_count"] - train_hourmean_trend[train_hourmean_trend$daylabel == u_daylabels[i],"transformed_count"]
}

# SmoothedLinearMD = lm(transformed_count ~ hour+workingday+weather+atemp+humidity+windspeed, data = train_residue)
SmoothedLinearMD = lm(transformed_count ~ workingday*hour + season +atemp+humidity+windspeed, data = train_residue)





## training loss
LlrLm_train = predict(SmoothedLinearMD,train) + predict(locfitmodel_hourmean_train, train$daylabel)
print(paste0("training loss: ",RMSLE_log(LlrLm_train, train$transformed_count)))

## validation loss
LlrLm_val = predict(SmoothedLinearMD,val) + predict(locfitmodel_hourmean_train, val$daylabel)
print(paste0("validation loss: ",RMSLE_log(LlrLm_val, val$transformed_count)))


## show how the fit goes 
plot(val$transformed_count[1:100])
lines(LlrLm_val[1:100], col = "blue")
lines(linearPredict_val,col = "red")
```




## (c) Using additive model
```{r}
gamMD <- gam(transformed_count ~ daylabel+workingday*hour+atemp*season + humidity+workingday+holiday*weather, data = train_residue)
#gamMD <- gam(transformed_count ~ daylabel+workingday+hour+workingday*hour + season +atemp+humidity+windspeed, data = train_residue)

## training loss
GamLm_train = predict(gamMD,train) + predict(locfitmodel_hourmean_train, train$daylabel)
print(paste0("training loss: ",RMSLE_log(GamLm_train, train$transformed_count)))

## validation loss
GamLm_val = predict(gamMD,val) + predict(locfitmodel_hourmean_train, val$daylabel)
print(paste0("validation loss: ",RMSLE_log(GamLm_val, val$transformed_count)))


## show how the fit goes 
plot(val$transformed_count[1:100])
lines(GamLm_val[1:100], col = "green")
lines(LlrLm_val[1:100], col = "blue")
lines(linearPredict_val,col = "red")
```

## Comment:
I got the best prediction from part (c). Though gam can lift my the model performance a little, feature selection is perhaps more important. Before adding the interaction term for workingday*hour, my best loss is around 1, much worse than the simplist linear model used here.

```{r}
## prediction on test
GamLm_test = predict(gamMD,test) + predict(locfitmodel_hourmean_train, test$daylabel)
write.table(as.numeric(GamLm_test), "../data/assn1-wangzh.txt", row.names = FALSE, col.names=FALSE, sep = "\n")
```




